---
layout: post
title:  "Offline RL in Regular Decision Processes: Sample Efficiency via Language Metrics"
date:   2025-02-24 22:21:59 +00:00
image: /images/ewrl.png
#image: /images2/cookie.png
categories: research
author: "Ahana Deb"
authors: "<strong>Ahana Deb</strong>, Roberto Cipollone, Anders Jonsson, Alessandro Ronca, Mohammad Sadegh Talebi"
venue: "ICLR (Poster)"
# arxiv: https://arxiv.org/abs/2409.02747v1
link: https://openreview.net/forum?id=EW6bNEqalF&nesting=2&sort=date-desc
# code: https://github.com/leonidk/fmb-plus
# website: https://leonidk.github.io/fmb-plus
# poster: /pdfs/EWRL_2024_Poster.pdf
---

This work studies offline Reinforcement Learning (RL) in a class of non-Markovian environments called Regular Decision Processes (RDPs). In RDPs, the unknown dependency of future observations and rewards from the past interactions can be captured by some hidden finite-state automaton. For this reason, many RDP algorithms first reconstruct this unknown dependency using automata learning techniques. In this paper, we consider episodic RDPs and show that it is possible to overcome the limitations of existing offline RL algorithms for RDPs via the introduction of two original techniques: a novel metric grounded in formal language theory and an approach based on Count-Min-Sketch (CMS). Owing to the novel language metric, our algorithm is proven to be more sample efficient than existing results, and in some problem instances admitting low complexity languages, the gain is showcased to be exponential in the episode length. The CMS-based approach removes the need for na√Øve counting and alleviates the memory requirements for long planning horizons. We derive Probably Approximately Correct (PAC) sample complexity bounds associated to each of these techniques, and validate the approach experimentally.