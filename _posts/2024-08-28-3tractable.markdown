---
layout: post
title:  "Tractable Offline Learning of Regular Decision Processes"
date:   2024-08-28 22:21:59 +00:00
image: /images/cookie.png
categories: research
author: "Ahana Deb"
authors: "<strong>Ahana Deb</strong>, Roberto Cipollone, Anders Jonsson, Alessandro Ronca, Mohammad Sadegh Talebi"
venue: "EWRL"
arxiv: https://arxiv.org/abs/2409.02747v1
# code: https://github.com/leonidk/fmb-plus
# website: https://leonidk.github.io/fmb-plus
poster: /pdfs/EWRL_2024_Poster.pdf
---
This work studies offline Reinforcement Learning (RL) in a class of non-Markovian environments called Regular Decision Processes (RDPs). In RDPs, the unknown dependency of future observations and rewards from the past interactions can be captured by some hidden finite-state automaton. For this reason, many RDP algorithms first reconstruct this unknown dependency using automata learning techniques. In this paper, we show that it is possible to overcome two strong limitations of previous offline RL algorithms for RDPs, notably RegORL. This can be accomplished via the introduction of two original techniques:
the development of a new pseudometric based on formal languages, which removes a problematic dependency on $L_\infty^p$-distinguishability parameters, and the adoption of Count-Min-Sketch (CMS), instead of naive counting. The former reduces the number of samples required in environments that are characterized by a low complexity in language-theoretic terms. The latter alleviates the memory requirements for long planning horizons. We derive the PAC sample complexity bounds associated to each of these techniques, and we validate the approach experimentally.